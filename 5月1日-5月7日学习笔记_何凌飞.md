---
title: 5月1日-5月7日学习笔记_何凌飞 
tags: 基础数学笔记,基础实战笔记
grammar_cjkRuby: true
grammar_mathjax: true
---

## **数学笔记**
#### 一元泰勒展开
$$ f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}f''(x_0)(x-x_0)^2+...+\frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n... $$
- 梯度下降法：展开到一次为止
- 牛顿法：展开到二次为止

#### 逆矩阵
- 逆矩阵为方阵
- 逆矩阵存在则左逆等于右逆
-  `!$(AB)^{-1}=B^{-1}A^{-1}$`
- `!$(A^{-1})^{-1}=A$`
- `!$(A^{T})^{-1}=(A^{-1})^{T}$`

#### 行列式
-  `!$|AB|=|A||B|$`
- `!$|A^{-1}|=|A|^{-1}$`
- `!$|\alpha A|=\alpha^n|A|$`

#### 矩阵的正定性（负定性）
- **n元二次型**
含有n个变量的二次齐次多项式,表示为`!$f(x_1,x_2,...x_n)=\sum_{i,j=1}^{n}a_{ij}x_ix_j$`
- **二次型矩阵**
对于n元二次型，取其系数矩阵，则有二次型矩阵A：
$$A=\begin{pmatrix}
a_{11} & \cdots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{n1} & \cdots & a_{nn}
\end{pmatrix}_{n\times n}$$
 可以看出，二次型矩阵一定是对称矩阵。
 - **另一种表示二次型的方式**
  $$f = x^TAx$$
- **正定矩阵**
	二次型中，对于任意非零向量x均有f>0，则其对于的二次型矩阵A为正定矩阵
- **半正定矩阵**
	二次型中，对于任意非零向量x均有f≥0，则其对于的二次型矩阵A为正定矩阵
- **负定矩阵**
	二次型中，对于任意非零向量x均有f<0，则其对于的二次型矩阵A为正定矩阵
- **半负定矩阵**
	二次型中，对于任意非零向量x均有f≤0，则其对于的二次型矩阵A为正定矩阵

- **==判定矩阵正定性最常用的方法==**
 特征值全部大于0

#### 梯度
- 设f(x)有n个自变量，这里的x是n个自变量组成的向量

则梯度表示为：$$\triangledown f(x)= (\frac{\partial f}{\partial x_1},...,\frac{\partial f}{\partial x_n})^T$$

#### 雅克比矩阵
- `!$y = f(x)$`    
- `!$y_i = f_i(x)$`
- x向量为n维，y向量为m维

则雅克比矩阵表示为：
$$\begin{pmatrix}
\frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\frac{\partial y_n}{\partial x_1} & \cdots & \frac{\partial y_n}{\partial x_n}
\end{pmatrix}_{m\times n}$$
还可以用各函数的梯度表示成:
$$\begin{pmatrix}
 （\triangledown y_1）^T\\
\vdots \\
 （\triangledown y_n）^T
\end{pmatrix}_{m\times 1}$$
#### Hessian ==/'hesiən/== 矩阵
- `!$y = f(x)$` 是n元函数，x为自变量构成的向量   

则Hessian矩阵表示为：
$$\begin{pmatrix}
\frac{\partial ^2f}{\partial x_1^2} & \cdots & \frac{\partial ^2f}{\partial x_1\partial x_n}\\
\vdots & \ddots & \vdots\\
\frac{\partial ^2f}{\partial x_n\partial x_1}& \cdots & \frac{\partial ^2f}{\partial x_n^2}
\end{pmatrix}_{n\times n}$$
- 由高阶偏导的性质得，Hessian矩阵是对称矩阵
- 若Hessian矩阵正定，则f(x)是凸函数；负定则是凹函数。

#### 知识点的结合应用 1-多元函数的泰勒展开
$$f(x)=f(x_0)+(\triangledown f(x_0))^T(x-x_0)+\frac{1}{2}(x-x_0)^TH(x-x_0)+o(||x-x_0||^2)$$
$$其中H为Hessian矩阵，注意和一元函数的泰勒展开类比记忆$$

#### 知识点的结合应用 2-求极值
- 若函数在某点的梯度等于`!$\overrightarrow{0}$`，则该点是函数的==驻点==。
- 再根据函数的==Hessian矩阵的正定或负定性==来判断是取极大值还是极小值。
- 正定：取极小值；负定：取极大值；不定：还需要看更高阶的导数。

#### 对角矩阵
 除对角线外（对角线可以0，也可以不为0），其他元素全为0的矩阵
#### 正交矩阵
对于方阵A,有$$A^T = A^{-1}$$则A就是正交矩阵

#### 特征值和特征向量相关概念（重点）
- **基本定义**
	设A为n阶==方阵==，若存在一个数λ及非零n维向量x，使得$$Ax= \lambda x$$成立，则称λ是矩阵A的==特征值==，x是A属于λ的一个==特征向量==
- **特征方程**
  - `!$由Ax = \lambda x得，（A-\lambda I)x=0.$`
  - `!$要求特征值，则要保证x可以取非零向量使得等式成立，即要使（A-\lambda I)x=0.$`对应的**齐次方程组有非零解**，也就是**解的个数大于1**，也就是矩阵**非满秩**，也就是**矩阵对应的行列式的值为0**
  - **由上述推导得出求特征值的方法**
    1. 数学算法：求|A-λI|=0得到的解λ为特征值，而|A-λI|=0称为==A的特征方程==
    2. 工程算法： QR算法
    
 - **求特征向量的方法**
	 将根据==特征方程==求得的==特征值==分别代入`!$(A-\lambda I)x=0.$`用==基础解系==表示特征向量
	 
- **迹运算**
  - 矩阵对角元素的和，表示为$$tr(A)=\sum_{i=1}^{n}a_{ii}$$

- **特征值的重要性质**
 $$\prod_{i=1}^{n}\lambda_i=|A|$$

- **矩阵的特征值分解**
	通过正交变换，把一个矩阵化为对角矩阵
	- 设A是待转化的方阵，P是对角矩阵，则有$$P^{-1}AP=\Lambda$$
$$其中\Lambda=\begin{pmatrix}
\lambda_{1} & \cdots & 0\\
\vdots & \ddots & \vdots\\
0& \cdots & \lambda_{n}
\end{pmatrix}$$
**特征值分解**：把正交变换等式转化一下，变成`!$A=P\Lambda P^{-1}$`,即把一个普通矩阵A，化为一个正交矩阵P、一个由A的特征值构成的对角矩阵`!$\Lambda和P^{-1}$`三者的积，就是矩阵A的特征值分解

#### 矩阵和向量中重要的几个求导公式

1. `!$\triangledown(w^Tx)=w$`

2. `!$\triangledown(x^TAx)=(A+A^T)x$`
	推导：$$由F=x^TAx=\sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j得其梯度\\\triangledown F=
	\begin{pmatrix}
\sum_{j=1}^{n}a_{1j}x_j+ \sum_{i=1}^{n}a_{i1}x_i\\
\vdots\\\sum_{j=1}^{n}a_{nj}x_j+ \sum_{i=1}^{n}a_{in}x_i 
\end{pmatrix}\\S=A+A^T=
\begin{pmatrix}
a_{11} +a_{11} & \cdots & a_{1n}+a_{n1}\\
\vdots & \ddots & \vdots\\
a_{1n}+a_{n1} & \cdots & a_{nn}+a_{nn}
\end{pmatrix}\\ Sx=\triangledown F\\得证 $$

3. `!$\triangledown^2(x^TAx)=A+A^T（注：这里的\triangledown^2指求Hessian矩阵）$` 

#### 奇异值分解
针对非方阵A的分解,公式为：$$A=U\Sigma V^T\\(其中U是对AA^T正交化的矩阵，V是对A^TA正交化的矩阵）$$

#### 贝叶斯公式
- 设a,b为两个事件，且a为因，b为果。
- 先验概率：p(b|a)
- 后验概率：p(a|b)
- 贝叶斯公式：即两个概率之间的关系$$p(a|b)=\frac{p(a)p(b|a)}{p(b)}$$

--------
## **实战笔记**



